{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22132040-b33f-474c-b056-774adc95062d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelos 03\n",
    "### Sistema Não-Binário\n",
    "#### Karla Pascoalini e Cauê Santos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf53ac-4579-40c5-a677-6283072bbf13",
   "metadata": {},
   "source": [
    "## Proponente: \n",
    "Diana Didatolov (Diana é relativamente nova para a posição de prestígio que ocupa na Biblioteca Real. Isso aconteceu pois ela teve um papel importantíssimo durante a Guerra das Marretas Voadoras, o que resultou na sua promoção dentro da Biblioteca. A diretora anterior, que perdeu seu cargo, é sua arquirrival por conta disso).\n",
    "\n",
    "## Objetivo:\n",
    "Se informe sobre outros modelos lineares que existem. Escolha um desses modelos e faça um notebook didático mostrando como este modelo funciona e como ele se difere do modelo linear estudado em sala de aula. Em um conjunto de dados da sua escolha, treine um modelo linear múltiplo e compare o resultado com o modelo linear que você escolheu. Sua comparação deve levar em conta alguma métrica (como RMSE, por exemplo). Gráficos são muito bem-vindos!\n",
    "\n",
    "### Comentários adicionais: \n",
    "O scikit-learn tem vários modelos lineares prontos, você pode conferir eles em https://scikit-learn.org/stable/modules/linear_model.html. A explicação aprofundada de cada modelo pode ser encontrada em livros e em vídeos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50721986-c28c-4d44-bc95-d38078d225ef",
   "metadata": {},
   "source": [
    "### Modelo de Regressão Bayesiana Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c9631-ed8d-4185-bad7-96d8ac6c652e",
   "metadata": {},
   "source": [
    "Diferente dos modelos que vimos anteriormente, este modelo não assume apenas um termo para levar em consideração a incerteza oriunda dos dados presentes, na verdade, vai até o final contabilizando que os dados terão valores diferentes. Entretanto, faz uma suposição importante para seu funcionamento: De que os dados seguem uma distribuição normal em sua diferença.\n",
    "\n",
    "A regressão Bayesiana Ridge funciona assim:\n",
    "\n",
    "- Primeiro, o modelo assume que os parâmetros da reta (a inclinação $\\beta_1$ e o ponto de intercecção $\\beta_0$) são distribuídos normalmente, ou seja, seguindo uma distribuição normal.\n",
    "- Em segundo lugar, o modelo usa esses parâmetros para fazer uma previsão baseado no conjunto de dados fornecidos\n",
    "- Em terceiro lugar, o modelo calcula a probabilidade de cada previsão estar correta.\n",
    "- Finalmente, o modelo usa essas probabilidades para calcular a melhor linha possível.\n",
    "\n",
    "Assim como vimos em probabilidade e estatística, o nome Bayesiana não é infundado, isso porque o modelo utiliza parte desse raciocínio para se \"auto-ajustar\" de certa forma. Quando ele calcula a probabilidade de sua previsão estar correta, é como se o modelo estivesse calculando a probabilidade do valor esperado, dado a previsão já feita.\n",
    "\n",
    "O termo \"Ridge\" vem do fato de que o modelo adiciona um termo de penalidade à função de custo. Esse termo de penalidade faz com que os parâmetros do modelo sejam menores. Isso ajuda a reduzir o superajuste, pois faz com que o modelo seja menos sensível a pequenas flutuações nos dados.\n",
    "\n",
    "Para esse modelo de aprendizado de máquina, utiliza-se a probabilidade como uma mira ou uma lente, capaz de focalizar melhor os esforços, afim de resultar em um melhor resultado no final. Também é possível adaptar os parâmetros enquanto estamos na fase de estimar os valores, além de que esse modelo pode ter um pouco mais de \"molejo\" com os dados, dada a sua natureza inerentemente probabilística.\n",
    "\n",
    "A regressão Bayesiana Ridge é um modelo que pode ser usada para reduzir o \"_overfitting_\" em modelos de regressão linear. Ela é frequentemente usada em aplicações onde os dados são ruidosos ou não lineares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b1a94fc-b981-4d5f-ad7f-86c0d198264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "DATASET_NAME = \"penguins\"\n",
    "FEATURES = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\n",
    "TARGET = [\"body_mass_g\"]\n",
    "\n",
    "df = sns.load_dataset(DATASET_NAME)\n",
    "\n",
    "df = df.reindex(FEATURES + TARGET, axis=1)\n",
    "df = df.dropna() \n",
    "\n",
    "X = df.reindex(FEATURES, axis=1)\n",
    "y = df.reindex(TARGET, axis=1)\n",
    "\n",
    "X = X.values\n",
    "y = y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a85f16a-cd63-47e2-be07-a88c66c0d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3204.15220769 2696.41646713]\n",
      "O MSE do modelo foi de 152998.96430792563 unidades de y ao quadrado\n",
      "O RMSE do modelo foi de 391.15082041065136 unidades de y\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "regplot inputs must be 1d",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m X\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     27\u001b[0m y\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m---> 29\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\venv\\ilumpy\\lib\\site-packages\\seaborn\\regression.py:899\u001b[0m, in \u001b[0;36mresidplot\u001b[1;34m(data, x, y, x_partial, y_partial, lowess, order, robust, dropna, label, color, scatter_kws, line_kws, ax)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresidplot\u001b[39m(\n\u001b[0;32m    839\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    840\u001b[0m     x_partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y_partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lowess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    841\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, robust\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    842\u001b[0m     scatter_kws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, line_kws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    843\u001b[0m ):\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;124;03m\"\"\"Plot the residuals of a linear regression.\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03m    This function will regress y on x (possibly as a robust or polynomial\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    897\u001b[0m \n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 899\u001b[0m     plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_RegressionPlotter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mci\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mx_partial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_partial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_partial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    905\u001b[0m         ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[1;32mc:\\venv\\ilumpy\\lib\\site-packages\\seaborn\\regression.py:107\u001b[0m, in \u001b[0;36m_RegressionPlotter.__init__\u001b[1;34m(self, x, y, data, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, seed, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, dropna, x_jitter, y_jitter, color, label)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMutually exclusive regression options.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Extract the data vals from the arguments or passed dataframe\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestablish_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mx_partial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_partial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_partial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Drop null observations\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropna:\n",
      "File \u001b[1;32mc:\\venv\\ilumpy\\lib\\site-packages\\seaborn\\regression.py:53\u001b[0m, in \u001b[0;36m_LinearPlotter.establish_variables\u001b[1;34m(self, data, **kws)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(vector) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     52\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregplot inputs must be 1d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, vector)\n",
      "\u001b[1;31mValueError\u001b[0m: regplot inputs must be 1d"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Realizar uma previsão\n",
    "x = [\n",
    "    [43, 20, 180],\n",
    "    [39, 20.5, 170],\n",
    "]\n",
    "previsao = reg.predict(x)\n",
    "\n",
    "previsao_correta = reg.predict(X)\n",
    "\n",
    "MSE = mean_squared_error(y, previsao_correta)\n",
    "RMSE = mean_squared_error(y, previsao_correta, squared = False)\n",
    "\n",
    "print(previsao)\n",
    "print(f'O MSE do modelo foi de {MSE} unidades de y ao quadrado')\n",
    "print(f'O RMSE do modelo foi de {RMSE} unidades de y')\n",
    "\n",
    "X.flatten()\n",
    "y.flatten()\n",
    "\n",
    "sns.residplot(x=X, y=y, data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2117899c-b90b-4950-935a-d506f10379f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O R² desse modelo é de 0.7614053386124096\n"
     ]
    }
   ],
   "source": [
    "print(f'O R² desse modelo é de {reg.score(X,y)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
